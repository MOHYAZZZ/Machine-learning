{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f94bd77",
   "metadata": {},
   "source": [
    "# Text Generation with Recurrent Neural Networks (RNN)\n",
    "\n",
    "In this notebook, we create a character-based Recurrent Neural Network (RNN) using TensorFlow. The RNN is trained on a portion of the Shakespeare dataset.\n",
    "\n",
    "The main steps of the project are:\n",
    "\n",
    "1. **Data Preparation:** \n",
    "    - Load the Shakespeare dataset from TensorFlow datasets.\n",
    "    - Preprocess the data by mapping strings to a numerical representation.\n",
    "\n",
    "2. **Model Definition:** \n",
    "    - Define a custom model class `MyModel` which inherits from `tf.keras.Model`. \n",
    "    - The model consists of three layers: an Embedding layer, a GRU layer, and a Dense layer.\n",
    "\n",
    "3. **Model Training:** \n",
    "    - Compile and train the model using the `fit` method. \n",
    "    - We also use a custom training loop to control the training process.\n",
    "\n",
    "4. **Text Generation:** \n",
    "    - After training, the model is used to generate new text. \n",
    "    - We use the `OneStep` model to generate text character by character.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b48a933b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf0f1d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa50fbf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "# Read the data\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "# Length of text is the number of characters in it\n",
    "print(f'Length of text: {len(text)} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ded620c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Have a look at the first 250 characters in text\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1aab118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 unique characters\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print(f'{len(vocab)} unique characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db23498b",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_texts = ['abcdefg', 'xyz']\n",
    "\n",
    "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
    "ids_from_chars = preprocessing.StringLookup(vocabulary=list(vocab), mask_token=None)\n",
    "\n",
    "ids = ids_from_chars(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcb4e28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
    "\n",
    "chars = chars_from_ids(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2aea2ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'abcdefg', b'xyz'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.reduce_join(chars, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8cd233c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_from_ids(ids):\n",
    "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be3386cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
      " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
      " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
      " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
      " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
      " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
      " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
      " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
    "\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "\n",
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for seq in sequences.take(1):\n",
    "  print(chars_from_ids(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f92598a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each sequence, duplicate and shift it to form the input and target text\n",
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32a1f0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each sequence, duplicate and shift it to form the input and target text\n",
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a48d6dd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23cc21d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "    super().__init__(self)\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                   return_sequences=True, \n",
    "                                   return_state=True)\n",
    "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "    x = inputs\n",
    "    x = self.embedding(x, training=training)\n",
    "    if states is None:\n",
    "      states = self.gru.get_initial_state(x)\n",
    "    x, states = self.gru(x, initial_state=states, training=training)\n",
    "    x = self.dense(x, training=training)\n",
    "\n",
    "    if return_state:\n",
    "      return x, states\n",
    "    else: \n",
    "      return x\n",
    "\n",
    "model = MyModel(\n",
    "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
    "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9e2a9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "162cf147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  16896     \n",
      "                                                                 \n",
      " gru (GRU)                   multiple                  3938304   \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  67650     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,022,850\n",
      "Trainable params: 4,022,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e4a19f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc9bdb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf136a19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "172/172 [==============================] - 328s 2s/step - loss: 2.6933\n",
      "Epoch 2/20\n",
      "172/172 [==============================] - 327s 2s/step - loss: 1.9689\n",
      "Epoch 3/20\n",
      "172/172 [==============================] - 328s 2s/step - loss: 1.6927\n",
      "Epoch 4/20\n",
      "172/172 [==============================] - 337s 2s/step - loss: 1.5356\n",
      "Epoch 5/20\n",
      "172/172 [==============================] - 352s 2s/step - loss: 1.4392\n",
      "Epoch 6/20\n",
      "172/172 [==============================] - 351s 2s/step - loss: 1.3726\n",
      "Epoch 7/20\n",
      "172/172 [==============================] - 377s 2s/step - loss: 1.3199\n",
      "Epoch 8/20\n",
      "172/172 [==============================] - 388s 2s/step - loss: 1.2757\n",
      "Epoch 9/20\n",
      "172/172 [==============================] - 394s 2s/step - loss: 1.2346\n",
      "Epoch 10/20\n",
      "172/172 [==============================] - 394s 2s/step - loss: 1.1941\n",
      "Epoch 11/20\n",
      "172/172 [==============================] - 385s 2s/step - loss: 1.1534\n",
      "Epoch 12/20\n",
      "172/172 [==============================] - 335s 2s/step - loss: 1.1119\n",
      "Epoch 13/20\n",
      "172/172 [==============================] - 338s 2s/step - loss: 1.0673\n",
      "Epoch 14/20\n",
      "172/172 [==============================] - 361s 2s/step - loss: 1.0220\n",
      "Epoch 15/20\n",
      "172/172 [==============================] - 336s 2s/step - loss: 0.9729\n",
      "Epoch 16/20\n",
      "172/172 [==============================] - 396s 2s/step - loss: 0.9219\n",
      "Epoch 17/20\n",
      "172/172 [==============================] - 414s 2s/step - loss: 0.8697\n",
      "Epoch 18/20\n",
      "172/172 [==============================] - 413s 2s/step - loss: 0.8188\n",
      "Epoch 19/20\n",
      "172/172 [==============================] - 7457s 44s/step - loss: 0.7680\n",
      "Epoch 20/20\n",
      "172/172 [==============================] - 332s 2s/step - loss: 0.7186\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc53bd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "    super().__init__()\n",
    "    self.temperature = temperature\n",
    "    self.model = model\n",
    "    self.chars_from_ids = chars_from_ids\n",
    "    self.ids_from_chars = ids_from_chars\n",
    "\n",
    "    # Create a mask to prevent \"\" or \"[UNK]\" from being generated.\n",
    "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "    # Run the model.\n",
    "    predicted_logits, states = self.model(inputs=input_ids, states=states, \n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "    # Apply the prediction mask: prevent \"\" or \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "630bdcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9164062b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "O the king's covil, speaking so: we\n",
      "will not prove a trunk return. But the argrument\n",
      "To use the glory of thy absence.\n",
      "What, will I be head?\n",
      "\n",
      "MIRANDA:\n",
      "O, let me live. I prithee, do you know.\n",
      "\n",
      "GLOUCESTER:\n",
      "My lord?\n",
      "\n",
      "KING RICHARD III:\n",
      "Ay, you rascless there! he's mutionman?\n",
      "O bowes nothing but from my soul awhile,\n",
      "And that be middow be much up, I pray,\n",
      "Anony on her: this cheer he knew the\n",
      "rest.\n",
      "\n",
      "JOHN OF GAUNT:\n",
      "Would then far from hence their loss.\n",
      "\n",
      "First Murderer:\n",
      "I part the Capitol; who set the lipsion,\n",
      "And then be it as the comforts from Lord\n",
      "Angelo poison any hereafter.\n",
      "\n",
      "MENENIUS:\n",
      "Dingle--\n",
      "Displace my raged with him to crave by thee:\n",
      "If I could speak again, or I'll keep him company.\n",
      "O, but not smile't one that shouldst thou fly!\n",
      "Where is the land the meat! Horse! Some durst on Antimble\n",
      "Writ in your authority command!'\n",
      "The suit we lie aside, and set down-sons'd.\n",
      "\n",
      "CLAUDIO:\n",
      "And come; what work is the forth and whereby he\n",
      "The fountast of them here and strays for vain.\n",
      "\n",
      "YORK:\n",
      "O easy, pomp!  \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 2.506497621536255\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "\n",
    "print(f\"\\nRun time: {end - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0b11aa5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAMLET:\n",
      "I joy with maid how the ground the forfeits house:\n",
      "And soother he that have you braced bedembrown.\n",
      "\n",
      "Second Gentleman:\n",
      "No, my lord.\n",
      "\n",
      "KING RICHARD III:\n",
      "Stanley, let's attend these news: the prepent device\n",
      "We shall be sworn trust, for he may call them;\n",
      "For there were none afone with thee and me.\n",
      "\n",
      "JULIET:\n",
      "I'll do thee jeason; that I leave yourself\n",
      "Lord march-take to my body to the queen;\n",
      "And yet no more than wives, these mother\n",
      "Hath brain'd the field in justice: then we see,\n",
      "I mean no memeed it theirs; 'the penitent drum\n",
      "Of our sortly, hath more confession absend;\n",
      "Such news, that you are in arms!\n",
      "If you do he, that our general: good night!\n",
      "Lead not your worship, for all thy edged,\n",
      "That form. Come hither, Master\n",
      "Froth do, and that thou wouldst disguised kisses; friend,\n",
      "Which the trim 'priam Lady? Happy stand; alack,\n",
      "That I, being gold their fears: is this folly in;\n",
      "When he is banish'd, I'll give him to the speech:\n",
      "One Pish false and your deceit, secure, free\n",
      "Of our own part, he hath full o \n",
      "\n",
      "________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "start_string = \"HAMLET:\"\n",
    "states = None\n",
    "next_char = tf.constant([start_string])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "51b9bfbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be or not to be, that is the question:\n",
      "Nothing the muchinard. Hese are the crown too: whether you lie!\n",
      "\n",
      "HORTENSIO:\n",
      "You're passing grandam too:\n",
      "To see her, fiery when I sin--for solicy?\n",
      "\n",
      "JULIET:\n",
      "But that you not pass to the Duke of Clifford?\n",
      "Happy and father, for thy lord,\n",
      "That thou go along with me that he doth enough.\n",
      "\n",
      "LORD WILLOUGHBY:\n",
      "Have done your lordship?\n",
      "\n",
      "ANTONIO:\n",
      "My injucal tames are we leave us to't.\n",
      "\n",
      "BRUTUS:\n",
      "Yes, but he cannot cheque him on\n",
      "A way as is the justice of a fomblice:\n",
      "I am so brief of mercy of the ministers,\n",
      "Repitation should be happite for thy hand,\n",
      "It is as happy by the secret mighty,\n",
      "Ere yea obey'dow, and took your honour,\n",
      "I'll prove a bark of janes but Bringbroke\n",
      "Fifth winds, two youth will show\n",
      "What's yet ungovern'd in puts base.\n",
      "\n",
      "OXFORD:\n",
      "Where doth he straight? and doth not, think no more: away!\n",
      "\n",
      "GONZALO:\n",
      "I'll wait upon your wanton wish: is pain'd\n",
      "The admired often person advanceful coals;\n",
      "For this is calved for the duke with graces, all the\n",
      "spigest, with him off, my conscience and let Shrewd's\n",
      "w \n",
      "\n",
      "________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "start_string = \"To be or not to be, that is the question:\"\n",
    "states = None\n",
    "next_char = tf.constant([start_string])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0014e4",
   "metadata": {},
   "source": [
    "> **Note:**\n",
    ">\n",
    "> The output of the model might not always make perfect sense, but it often has coherent phrases and sometimes even complete sentences that seem Shakespearean, which is a testament to the power of RNNs for sequence generation tasks.\n",
    ">\n",
    "> This model is stochastic, meaning each time you run the model you'll get slightly different results as it involves randomness in its predictions. You can experiment with different seed strings, adjust the temperature parameter in the OneStep model (a higher temperature results in more random output, a lower temperature results in more predictable output), or even train the model for more epochs to see how it affects the generated output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f2f8db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
